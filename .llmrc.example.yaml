# If this is enabled, the streaming mode (use_streaming) will be disabled
always_copy: false # Default: false

# Syntax highlight the LLM output
always_format: false # Default: false

# OpenRouter API key
api_key: ""

# The LLM model to use for chat
model: google/gemini-2.5-flash # Default: google/gemini-2.5-flash
models:
  # Short names for your models
  aliases:
    10x: anthropic/claude-sonnet-4
    fast: openai/gpt-4.1-nano
    gpt4: openai/gpt-4o
    smart: google/gemini-2.5-pro

# Show the LLM output in a streaming way. If set to false, it'll
# show it in a blocking way, that's wait for all the text to be available then show it
use_streaming: true # Default: true

verbose: false # Default: false
debug_mode: false # Default: false
log_file: "" # Default: "~/.llm/"
